"""
Servi√ßo de IA - Integra√ß√£o com Pydantic AI
Fornece capacidades de IA para agentes e chat
Seguindo padr√£o IT Valley Architecture
"""
import logging
import os
from typing import Optional, Dict, Any
from pydantic_ai import Agent
from pydantic import BaseModel

from config.settings import settings
from integrations.ai.rag_agent import RagAgentRunner

logger = logging.getLogger(__name__)


class AIResponse(BaseModel):
    """Resposta padr√£o da IA"""
    response: str


# Singleton para AIService (evita m√∫ltiplas inicializa√ß√µes do Pinecone)
_ai_service_instance: Optional['AIService'] = None


class AIService:
    """
    Servi√ßo de IA - Orquestra integra√ß√£o com modelos de IA
    Usa Pydantic AI como abstra√ß√£o de modelos
    Singleton pattern para evitar m√∫ltiplas inicializa√ß√µes do Pinecone
    """
    
    def __init__(self):
        """Inicializa o servi√ßo de IA com configura√ß√µes globais"""
        self.openai_api_key = settings.openai_api_key
        # Configura a vari√°vel de ambiente para Pydantic AI
        if self.openai_api_key:
            os.environ["OPENAI_API_KEY"] = self.openai_api_key
        
        if not self.openai_api_key:
            logger.warning("OPENAI_API_KEY n√£o configurada. IA n√£o funcionar√°.")

        # Usa pinecone_index_name se definido, sen√£o usa vector_db_index_name (padr√£o "employee")
        rag_index_name = settings.pinecone_index_name or settings.vector_db_index_name
        self.supports_rag = bool(settings.pinecone_api_key and rag_index_name)
        self.rag_runner: Optional[RagAgentRunner] = None
        if self.supports_rag:
            try:
                logger.info("üîß Inicializando RagAgentRunner (√≠ndice: %s)", rag_index_name)
                self.rag_runner = RagAgentRunner(rag_index_name)
                logger.info("‚úÖ RagAgentRunner inicializado com sucesso")
            except Exception as exc:
                logger.error("Falha ao inicializar RagAgentRunner: %s", exc, exc_info=True)
                self.supports_rag = False
    
    @classmethod
    def get_instance(cls) -> 'AIService':
        """
        Retorna inst√¢ncia singleton do AIService.
        Evita m√∫ltiplas inicializa√ß√µes do Pinecone.
        """
        global _ai_service_instance
        if _ai_service_instance is None:
            _ai_service_instance = cls()
        return _ai_service_instance
    
    async def generate_response(
        self,
        message: str,
        system_prompt: str = "Voc√™ √© um assistente √∫til e amig√°vel.",
        agent_id: Optional[str] = None,
        user_id: Optional[str] = None,
        temperature: float = 0.7,
        model: str = "gpt-4-turbo-preview",
        max_tokens: int = 2000
    ) -> str:
        """
        Gera resposta usando modelo de IA via Pydantic AI (ASYNC)
        
        Args:
            message: Mensagem do usu√°rio
            system_prompt: Prompt do sistema/agente
            agent_id: ID do agente (para logging)
            user_id: ID do usu√°rio (para logging)
            temperature: Criatividade da resposta (0-1)
            model: Modelo a usar
            max_tokens: M√°ximo de tokens na resposta
            
        Returns:
            str: Resposta gerada pela IA
            
        Raises:
            ValueError: Se API key n√£o configurada
        """
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY n√£o configurada")
        
        try:
            logger.info(f"ü§ñ Gerando resposta com IA para usu√°rio={user_id}, agente={agent_id}")
            logger.debug(f"   message={message[:100]}... model={model}")
            
            # Cria agent do Pydantic AI com tipo de sa√≠da definido
            agent = Agent(
                model=f"openai:{model}",
                system_prompt=system_prompt,
                output_type=AIResponse
            )
            
            # Executa agent de forma async
            result = await agent.run(message)
            
            # Extrai resposta
            response_text = result.data.response if hasattr(result.data, 'response') else str(result.data)
            
            logger.info(f"‚úÖ Resposta gerada com sucesso. Tokens aproximados: {len(response_text.split())}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar resposta com IA: {str(e)}", exc_info=True)
            raise

    def generate_rag_response_sync(
        self,
        message: str,
        agent_id: str,
        agent_name: str,
        instructions: Optional[str]
    ) -> str:
        """
        Executa resposta usando agente RAG quando suportado.
        """
        if not self.rag_runner:
            return self.generate_response_sync(
                message=message,
                system_prompt=instructions or f"Voc√™ √© o agente {agent_name}.",
                agent_id=agent_id,
                user_id=None
            )

        try:
            logger.info("üîé Executando fluxo RAG para agente %s", agent_id)
            return self.rag_runner.run(
                agent_id=agent_id,
                agent_name=agent_name,
                instructions=instructions,
                user_message=message,
            )
        except Exception as exc:
            logger.error("Erro no fluxo RAG, fallback para resposta padr√£o: %s", exc, exc_info=True)
            return self.generate_response_sync(
                message=message,
                system_prompt=instructions or f"Voc√™ √© o agente {agent_name}.",
                agent_id=agent_id,
                user_id=None
            )
    
    def generate_response_sync(
        self,
        message: str,
        system_prompt: str = "Voc√™ √© um assistente √∫til e amig√°vel.",
        agent_id: Optional[str] = None,
        user_id: Optional[str] = None,
        temperature: float = 0.7,
        model: str = "gpt-4-turbo-preview",
        max_tokens: int = 2000
    ) -> str:
        """
        Gera resposta usando modelo de IA via Pydantic AI (SYNC)
        Vers√£o s√≠ncrona para uso em contextos n√£o-async
        
        Args:
            message: Mensagem do usu√°rio
            system_prompt: Prompt do sistema/agente
            agent_id: ID do agente (para logging)
            user_id: ID do usu√°rio (para logging)
            temperature: Criatividade da resposta (0-1)
            model: Modelo a usar
            max_tokens: M√°ximo de tokens na resposta
            
        Returns:
            str: Resposta gerada pela IA
            
        Raises:
            ValueError: Se API key n√£o configurada
        """
        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY n√£o configurada")
        
        try:
            logger.info(f"ü§ñ Gerando resposta com IA para usu√°rio={user_id}, agente={agent_id}")
            logger.debug(f"   message={message[:100]}... model={model}")
            
            # Cria agent do Pydantic AI com tipo de sa√≠da definido
            agent = Agent(
                model=f"openai:{model}",
                system_prompt=system_prompt,
                output_type=AIResponse
            )
            
            # Executa agent de forma s√≠ncrona
            result = agent.run_sync(message)
            
            # Extrai resposta
            response_text = result.data.response if hasattr(result.data, 'response') else str(result.data)
            
            logger.info(f"‚úÖ Resposta gerada com sucesso. Tokens aproximados: {len(response_text.split())}")
            
            return response_text
            
        except Exception as e:
            logger.error(f"‚ùå Erro ao gerar resposta com IA: {str(e)}", exc_info=True)
            raise
    
    async def process_agent_message(
        self,
        message: str,
        agent_config: Dict[str, Any],
        conversation_history: Optional[list] = None
    ) -> str:
        """
        Processa mensagem usando configura√ß√£o espec√≠fica do agente
        
        Args:
            message: Mensagem do usu√°rio
            agent_config: Configura√ß√£o do agente (name, system_prompt, model, temperature, etc)
            conversation_history: Hist√≥rico de conversa anterior
            
        Returns:
            str: Resposta gerada
        """
        system_prompt = agent_config.get('system_prompt', f"Voc√™ √© um assistente chamado {agent_config.get('name', 'Agente')}.")
        model = agent_config.get('model', 'gpt-4-turbo-preview')
        temperature = float(agent_config.get('temperature', 0.7))
        max_tokens = int(agent_config.get('max_tokens', 2000))
        
        # Monta contexto com hist√≥rico se dispon√≠vel
        full_message = message
        if conversation_history:
            # Limita hist√≥rico aos √∫ltimos 5 mensagens para economizar tokens
            recent_history = conversation_history[-10:]
            history_text = "\n".join([
                f"{h['role']}: {h['content']}"
                for h in recent_history
            ])
            full_message = f"Hist√≥rico da conversa:\n{history_text}\n\nNova mensagem do usu√°rio: {message}"
        
        return await self.generate_response(
            message=full_message,
            system_prompt=system_prompt,
            temperature=temperature,
            model=model,
            max_tokens=max_tokens
        )
